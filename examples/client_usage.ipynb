{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EvalHub Client SDK Usage Examples\n",
    "\n",
    "This notebook demonstrates how to use the EvalHub client SDK for interacting with the EvalHub evaluation service.\n",
    "\n",
    "The SDK provides separate client classes for async and sync operations:\n",
    "- `AsyncEvalHubClient` - For asynchronous operations (recommended for I/O-bound workloads)\n",
    "- `SyncEvalHubClient` - For synchronous operations\n",
    "\n",
    "Both use a **nested resource structure**:\n",
    "- `client.providers` - Provider operations\n",
    "- `client.benchmarks` - Benchmark operations\n",
    "- `client.collections` - Collection operations\n",
    "- `client.jobs` - Evaluation job operations\n",
    "\n",
    "## Prerequisites: Set Environment Variables\n",
    "\n",
    "Before running this notebook, set the required environment variables in your shell:\n",
    "\n",
    "```bash\n",
    "# Get your OpenShift authentication token\n",
    "export EVALHUB_TOKEN=$(oc whoami -t)\n",
    "\n",
    "# Get the EvalHub route URL from your namespace (e.g., 'test')\n",
    "export EVALHUB_URL=https://$(oc get route evalhub -n test -o jsonpath='{.spec.host}')\n",
    "\n",
    "# Verify the variables are set\n",
    "echo \"Token: ${EVALHUB_TOKEN:0:20}...\"\n",
    "echo \"URL: $EVALHUB_URL\"\n",
    "```\n",
    "\n",
    "Then start Jupyter with these variables available:\n",
    "```bash\n",
    "jupyter notebook client_usage.ipynb\n",
    "```\n",
    "\n",
    "## OpenShift Authentication\n",
    "\n",
    "The SDK has built-in support for OpenShift authentication with three methods:\n",
    "\n",
    "1. **ServiceAccount Token** (`auth_token_path`): For pods running inside OpenShift, use the automounted token at `/var/run/secrets/kubernetes.io/serviceaccount/token`\n",
    "2. **User Token** (`auth_token`): For local development, use token from `oc whoami -t` (shown above)\n",
    "3. **Environment Variable** (`auth_token`): For production, use token from environment variable or secret (used in examples below)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup: Import Required Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evalhub import (\n",
    "    AsyncEvalHubClient,\n",
    "    SyncEvalHubClient,\n",
    "    JobSubmissionRequest,\n",
    "    BenchmarkConfig,\n",
    "    ModelConfig,\n",
    ")\n",
    "import os\n",
    "\n",
    "# Verify environment variables are set\n",
    "if not os.getenv(\"EVALHUB_TOKEN\"):\n",
    "    raise ValueError(\"EVALHUB_TOKEN environment variable is not set. See Prerequisites section above.\")\n",
    "if not os.getenv(\"EVALHUB_URL\"):\n",
    "    raise ValueError(\"EVALHUB_URL environment variable is not set. See Prerequisites section above.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1: Basic Client Usage\n",
    "\n",
    "Connect to EvalHub and check health status."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TLS verification disabled - skipping CA bundle detection\n",
      "TLS verification disabled (insecure mode)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ EvalHub is healthy: healthy\n",
      "  Version: unknown\n",
      "  Uptime: 0.0s\n"
     ]
    }
   ],
   "source": [
    "# Create synchronous client with environment variable authentication\n",
    "with SyncEvalHubClient(\n",
    "    base_url=os.getenv(\"EVALHUB_URL\"),\n",
    "    auth_token=os.getenv(\"EVALHUB_TOKEN\"),\n",
    "    insecure=True,  # For self-signed certs (use ca_bundle_path in production)\n",
    ") as client:\n",
    "    try:\n",
    "        # Check health\n",
    "        health = client.health()\n",
    "        print(f\"✓ EvalHub is healthy: {health['status']}\")\n",
    "        print(f\"  Version: {health.get('version', 'unknown')}\")\n",
    "        print(f\"  Uptime: {health.get('uptime_seconds', 0):.1f}s\")\n",
    "    except Exception as e:\n",
    "        print(f\"✗ Failed to connect: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: List Providers and Benchmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TLS verification disabled - skipping CA bundle detection\n",
      "TLS verification disabled (insecure mode)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Found 5 providers:\n",
      "  - ragas: RAGAS\n",
      "  - garak: Garak\n",
      "  - guidellm: GuideLLM\n",
      "  - lighteval: Lighteval\n",
      "  - lm_evaluation_harness: LM Evaluation Harness\n",
      "\n",
      "✓ Found 4 benchmarks for ragas:\n",
      "  - faithfulness: Faithfulness\n",
      "  - answer_relevancy: Answer Relevancy\n",
      "  - context_precision: Context Precision\n",
      "  - context_recall: Context Recall\n"
     ]
    }
   ],
   "source": [
    "# List available providers using nested resource structure\n",
    "with SyncEvalHubClient(\n",
    "    base_url=os.getenv(\"EVALHUB_URL\"),\n",
    "    auth_token=os.getenv(\"EVALHUB_TOKEN\"),\n",
    "    insecure=True,\n",
    ") as client:\n",
    "    try:\n",
    "        # List all providers\n",
    "        providers = client.providers.list()\n",
    "        print(f\"✓ Found {len(providers)} providers:\")\n",
    "        for provider in providers:\n",
    "            print(f\"  - {provider.id}: {provider.name}\")\n",
    "        \n",
    "        # List benchmarks from a specific provider\n",
    "        if providers:\n",
    "            provider_id = providers[0].id\n",
    "            benchmarks = client.benchmarks.list(provider_id=provider_id)\n",
    "            print(f\"\\n✓ Found {len(benchmarks)} benchmarks for {provider_id}:\")\n",
    "            for benchmark in benchmarks[:5]:  # Show first 5\n",
    "                print(f\"  - {benchmark.id}: {benchmark.name}\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"✗ Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Submit an Evaluation Job\n",
    "\n",
    "**Important:** The `model.url` should be the **base URL only** (host:port), not including the `/v1/completions` endpoint path. The endpoint is added automatically by the evaluation framework.\n",
    "\n",
    "To find available services in your namespace:\n",
    "```bash\n",
    "# List services in your namespace\n",
    "oc get svc -n test\n",
    "\n",
    "# Example: If you have a vLLM service named 'vllm-server'\n",
    "# Correct URL format: http://vllm-server.test.svc.cluster.local:8000\n",
    "# WRONG format: http://vllm-server.test.svc.cluster.local:8000/v1/completions\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TLS verification disabled - skipping CA bundle detection\n",
      "TLS verification disabled (insecure mode)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Job submitted successfully\n",
      "  Job ID: 30184068-b829-4827-8446-ee3a4359f62d\n",
      "  State: unknown\n",
      "  Created: 2026-02-08 17:52:08.086035+00:00\n",
      "\n",
      "✓ Current job state: pending\n",
      "  Message: Evaluation job created\n"
     ]
    }
   ],
   "source": [
    "# Submit an evaluation job\n",
    "with SyncEvalHubClient(\n",
    "    base_url=os.getenv(\"EVALHUB_URL\"),\n",
    "    auth_token=os.getenv(\"EVALHUB_TOKEN\"),\n",
    "    insecure=True,\n",
    ") as client:\n",
    "    try:\n",
    "        # Create model configuration\n",
    "        # IMPORTANT: URL should be base URL only (host:port), NOT including /v1/completions\n",
    "        # The /v1/completions endpoint is added automatically by lm-eval\n",
    "        model = ModelConfig(\n",
    "            url=\"http://vllm-server.test.svc.cluster.local:8000\",\n",
    "            name=\"tinyllama\"\n",
    "        )\n",
    "        \n",
    "        # Create benchmark configuration\n",
    "        benchmark = BenchmarkConfig(\n",
    "            id=\"arc_easy\",\n",
    "            provider_id=\"lm_evaluation_harness\",\n",
    "            parameters={\n",
    "                \"limit\": 5,  # Number of examples to evaluate (use small number for testing)\n",
    "                \"tokenizer\": \"google/flan-t5-small\"  # HuggingFace tokenizer for the model\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        # Create job submission request\n",
    "        job_request = JobSubmissionRequest(\n",
    "            model=model,\n",
    "            benchmarks=[benchmark],\n",
    "            timeout_minutes=30,\n",
    "            retry_attempts=1\n",
    "        )\n",
    "        \n",
    "        # Submit job using nested resource\n",
    "        job = client.jobs.submit(job_request)\n",
    "        \n",
    "        # Store job ID for later use\n",
    "        submitted_job_id = job.id\n",
    "        \n",
    "        print(f\"✓ Job submitted successfully\")\n",
    "        print(f\"  Job ID: {submitted_job_id}\")\n",
    "        print(f\"  State: {job.state}\")\n",
    "        print(f\"  Created: {job.resource.created_at}\")\n",
    "        \n",
    "        # Check status\n",
    "        updated_job = client.jobs.get(submitted_job_id)\n",
    "        print(f\"\\n✓ Current job state: {updated_job.state}\")\n",
    "        if updated_job.status and updated_job.status.message:\n",
    "            print(f\"  Message: {updated_job.status.message.message}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"✗ Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Async Client Usage\n",
    "\n",
    "Using the asynchronous client for better performance with I/O-bound operations.\n",
    "\n",
    "**Note:** Same method names as sync - just await them!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Wait for Job Completion and Retrieve Results\n",
    "\n",
    "Monitor a job until completion and retrieve the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TLS verification disabled - skipping CA bundle detection\n",
      "TLS verification disabled (insecure mode)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for job 30184068-b829-4827-8446-ee3a4359f62d to complete...\n",
      "✓ Job completed with state: completed\n",
      "\n",
      "✓ Evaluation Results:\n",
      "  Total evaluations: 0\n",
      "  Completed: 0\n",
      "  Failed: 0\n",
      "\n",
      "✓ Benchmark Results:\n",
      "\n",
      "  Benchmark: arc_easy (Provider: )\n",
      "    Metrics:\n",
      "      - acc: 0.4\n",
      "      - acc_norm: 0.6\n",
      "      - acc_norm_stderr: 0.24494897427831783\n",
      "      - acc_stderr: 0.24494897427831783\n",
      "    Artifacts: 3 items\n",
      "\n",
      "✓ Benchmark Execution Status:\n",
      "  - arc_easy: completed\n"
     ]
    }
   ],
   "source": [
    "# Wait for job completion and retrieve results\n",
    "# This uses the submitted_job_id from the previous cell\n",
    "with SyncEvalHubClient(\n",
    "    base_url=os.getenv(\"EVALHUB_URL\"),\n",
    "    auth_token=os.getenv(\"EVALHUB_TOKEN\"),\n",
    "    insecure=True,\n",
    ") as client:\n",
    "    try:\n",
    "        # Use the job ID from the previous submission\n",
    "        # Make sure you ran the job submission cell first!\n",
    "        print(f\"Waiting for job {submitted_job_id} to complete...\")\n",
    "        \n",
    "        completed_job = client.jobs.wait_for_completion(\n",
    "            job_id=submitted_job_id,\n",
    "            timeout=600,  # Wait up to 10 minutes\n",
    "            poll_interval=5.0  # Check every 5 seconds\n",
    "        )\n",
    "        \n",
    "        print(f\"✓ Job completed with state: {completed_job.state}\")\n",
    "        \n",
    "        # Display results if available\n",
    "        if completed_job.results:\n",
    "            print(f\"\\n✓ Evaluation Results:\")\n",
    "            print(f\"  Total evaluations: {completed_job.results.total_evaluations}\")\n",
    "            print(f\"  Completed: {completed_job.results.completed_evaluations}\")\n",
    "            print(f\"  Failed: {completed_job.results.failed_evaluations}\")\n",
    "            \n",
    "            # Show benchmark results\n",
    "            if completed_job.results.benchmarks:\n",
    "                print(f\"\\n✓ Benchmark Results:\")\n",
    "                for bench_result in completed_job.results.benchmarks:\n",
    "                    print(f\"\\n  Benchmark: {bench_result.id} (Provider: {bench_result.provider_id})\")\n",
    "                    \n",
    "                    # Display metrics\n",
    "                    if bench_result.metrics:\n",
    "                        print(f\"    Metrics:\")\n",
    "                        for metric_name, metric_value in bench_result.metrics.items():\n",
    "                            print(f\"      - {metric_name}: {metric_value}\")\n",
    "                    \n",
    "                    # Display artifacts info if available\n",
    "                    if bench_result.artifacts:\n",
    "                        print(f\"    Artifacts: {len(bench_result.artifacts)} items\")\n",
    "                    \n",
    "                    # Display MLFlow info if available\n",
    "                    if bench_result.mlflow_run_id:\n",
    "                        print(f\"    MLFlow Run ID: {bench_result.mlflow_run_id}\")\n",
    "                    if bench_result.logs_path:\n",
    "                        print(f\"    Logs: {bench_result.logs_path}\")\n",
    "            \n",
    "            # Display MLFlow experiment URL if available\n",
    "            if completed_job.results.mlflow_experiment_url:\n",
    "                print(f\"\\n  MLFlow Experiment: {completed_job.results.mlflow_experiment_url}\")\n",
    "        else:\n",
    "            print(f\"\\n✗ No results available yet\")\n",
    "        \n",
    "        # Show benchmark execution status\n",
    "        if completed_job.status and completed_job.status.benchmarks:\n",
    "            print(f\"\\n✓ Benchmark Execution Status:\")\n",
    "            for bench in completed_job.status.benchmarks:\n",
    "                print(f\"  - {bench.id}: {bench.state}\")\n",
    "                if bench.message:\n",
    "                    print(f\"    Message: {bench.message.message}\")\n",
    "    \n",
    "    except NameError:\n",
    "        print(f\"✗ Error: submitted_job_id not found. Please run the job submission cell first!\")\n",
    "    except TimeoutError as e:\n",
    "        print(f\"✗ Job did not complete within timeout: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"✗ Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TLS verification disabled - skipping CA bundle detection\n",
      "TLS verification disabled (insecure mode)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Async health check: healthy\n",
      "✓ Found 5 providers (async)\n",
      "✓ Found 23 lighteval benchmarks (async)\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "\n",
    "async def async_example():\n",
    "    \"\"\"Demonstrate async client usage.\"\"\"\n",
    "    async with AsyncEvalHubClient(\n",
    "        base_url=os.getenv(\"EVALHUB_URL\"),\n",
    "        auth_token=os.getenv(\"EVALHUB_TOKEN\"),\n",
    "        insecure=True,\n",
    "    ) as client:\n",
    "        try:\n",
    "            # Async health check - same method name!\n",
    "            health = await client.health()\n",
    "            print(f\"✓ Async health check: {health['status']}\")\n",
    "            \n",
    "            # Async provider list - same method name!\n",
    "            providers = await client.providers.list()\n",
    "            print(f\"✓ Found {len(providers)} providers (async)\")\n",
    "            \n",
    "            # Async benchmark list - same method name!\n",
    "            benchmarks = await client.benchmarks.list(provider_id=\"lighteval\")\n",
    "            print(f\"✓ Found {len(benchmarks)} lighteval benchmarks (async)\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"✗ Async operation failed: {e}\")\n",
    "\n",
    "# Run the async example\n",
    "await async_example()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary: Authentication Methods for OpenShift\n",
    "\n",
    "The SDK supports three authentication methods:\n",
    "\n",
    "### 1. ServiceAccount Token (For Pods)\n",
    "Use `auth_token_path=\"/var/run/secrets/kubernetes.io/serviceaccount/token\"` when running inside OpenShift pods.\n",
    "\n",
    "### 2. User Token (For Development)\n",
    "Get token from `oc whoami -t` and pass as `auth_token=token` for local development.\n",
    "\n",
    "### 3. Environment Variable (Production - Used in Examples Above)\n",
    "Store token in environment variable and pass as `auth_token=os.getenv(\"EVALHUB_TOKEN\")`.\n",
    "\n",
    "**Example shown above:**\n",
    "```python\n",
    "with SyncEvalHubClient(\n",
    "    base_url=os.getenv(\"EVALHUB_URL\"),\n",
    "    auth_token=os.getenv(\"EVALHUB_TOKEN\"),\n",
    "    insecure=True,  # or ca_bundle_path=\"/path/to/ca.crt\"\n",
    ") as client:\n",
    "    providers = client.providers.list()\n",
    "```\n",
    "\n",
    "All three methods work identically - choose based on your deployment environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retry Logic and Error Handling\n",
    "\n",
    "The client includes automatic retry logic with exponential backoff for handling transient failures:\n",
    "\n",
    "- **Retried errors**: Connection timeouts, server errors (5xx), rate limits (429)\n",
    "- **Retry strategy**: Exponential backoff with jitter\n",
    "- **Default retries**: Up to 3 attempts with increasing delays\n",
    "- **Timeout**: Configurable per-request timeout (default: 30 seconds)\n",
    "\n",
    "The retry logic is built into the HTTP client and handles:\n",
    "- Network connection failures\n",
    "- Temporary server unavailability\n",
    "- Rate limiting responses\n",
    "- Gateway timeouts\n",
    "\n",
    "You don't need to configure anything - it works automatically!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced: Custom Timeout and SSL Configuration\n",
    "\n",
    "Configure custom timeouts or SSL certificate validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TLS verification disabled - skipping CA bundle detection\n",
      "TLS verification disabled (insecure mode)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Connected with custom configuration\n",
      "  Found 5 providers\n"
     ]
    }
   ],
   "source": [
    "# Example: Custom timeout and SSL configuration\n",
    "with SyncEvalHubClient(\n",
    "    base_url=os.getenv(\"EVALHUB_URL\"),\n",
    "    auth_token=os.getenv(\"EVALHUB_TOKEN\"),\n",
    "    \n",
    "    # Option 1: Disable SSL verification (development only)\n",
    "    insecure=True,\n",
    "    \n",
    "    # Option 2: Provide CA bundle path (production)\n",
    "    # ca_bundle_path=\"/path/to/ca-bundle.crt\",\n",
    "    \n",
    "    # Custom timeout (in seconds)\n",
    "    timeout=60.0,\n",
    ") as client:\n",
    "    try:\n",
    "        providers = client.providers.list()\n",
    "        print(f\"✓ Connected with custom configuration\")\n",
    "        print(f\"  Found {len(providers)} providers\")\n",
    "    except Exception as e:\n",
    "        print(f\"✗ Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
