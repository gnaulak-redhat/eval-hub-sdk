{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EvalHub Client SDK Usage Examples\n",
    "\n",
    "This notebook demonstrates how to use the EvalHub client SDK for interacting with the EvalHub evaluation service.\n",
    "\n",
    "The SDK provides separate client classes for async and sync operations:\n",
    "- `AsyncEvalHubClient` - For asynchronous operations (recommended for I/O-bound workloads)\n",
    "- `SyncEvalHubClient` - For synchronous operations\n",
    "\n",
    "Both use a **nested resource structure** similar to Llama Stack Client:\n",
    "- `client.providers` - Provider operations\n",
    "- `client.benchmarks` - Benchmark operations\n",
    "- `client.collections` - Collection operations\n",
    "- `client.jobs` - Evaluation job operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup: Import Required Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evalhub import (\n",
    "    AsyncEvalHubClient,\n",
    "    EvaluationRequest,\n",
    "    ModelConfig,\n",
    "    SyncEvalHubClient,\n",
    ")\n",
    "import asyncio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1: Synchronous Client Usage\n",
    "\n",
    "Basic usage with the synchronous client. This is the simplest way to get started."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create synchronous client (defaults to http://localhost:8080)\n",
    "with SyncEvalHubClient() as client:\n",
    "    # Check health\n",
    "    try:\n",
    "        health = client.health()\n",
    "        print(f\"✓ EvalHub is healthy: {health}\")\n",
    "    except Exception as e:\n",
    "        print(f\"✗ Failed to connect to local EvalHub: {e}\")\n",
    "\n",
    "    # List available providers using nested resource\n",
    "    try:\n",
    "        providers = client.providers.list()\n",
    "        print(f\"\\n✓ Found {len(providers)} providers\")\n",
    "        for provider in providers[:3]:  # Show first 3\n",
    "            print(f\"  - {provider.id}\")\n",
    "    except Exception as e:\n",
    "        print(f\"✗ Failed to list providers: {e}\")\n",
    "\n",
    "    # List available benchmarks using nested resource\n",
    "    try:\n",
    "        benchmarks = client.benchmarks.list(category=\"math\")\n",
    "        print(f\"\\n✓ Found {len(benchmarks)} math benchmarks\")\n",
    "        for benchmark in benchmarks[:3]:  # Show first 3\n",
    "            print(f\"  - {benchmark.benchmark_id}: {benchmark.name}\")\n",
    "    except Exception as e:\n",
    "        print(f\"✗ Failed to list benchmarks: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 2: Remote EvalHub Connection\n",
    "\n",
    "Connect to a remote EvalHub instance with authentication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create client for remote instance with authentication\n",
    "with SyncEvalHubClient(\n",
    "    base_url=\"https://evalhub.example.com\",\n",
    "    auth_token=\"your-api-token-here\",\n",
    "    timeout=60.0,\n",
    ") as client:\n",
    "    print(\"✓ Remote client created (connection would be tested on first API call)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 3: Submit an Evaluation Job\n",
    "\n",
    "Submit an evaluation job and check its status."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with SyncEvalHubClient() as client:\n",
    "    # Create evaluation request\n",
    "    # Using a vLLM endpoint deployed on OpenShift\n",
    "    request = EvaluationRequest(\n",
    "        benchmark_id=\"gsm8k\",\n",
    "        model=ModelConfig(\n",
    "            url=\"http://vllm-service.my-namespace.svc.cluster.local:8000/v1\",\n",
    "            name=\"meta-llama/Llama-2-7b-chat-hf\",\n",
    "        ),\n",
    "        num_few_shot=5,\n",
    "        experiment_name=\"GSM8K Evaluation\",\n",
    "        tags={\"environment\": \"dev\", \"version\": \"v1\"},\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        # Submit job using nested resource\n",
    "        job = client.jobs.submit(request)\n",
    "        print(f\"✓ Job submitted: {job.id}\")\n",
    "        print(f\"  Status: {job.status}\")\n",
    "\n",
    "        # Check status using nested resource\n",
    "        updated_job = client.jobs.get(job.id)\n",
    "        print(f\"✓ Job status updated: {updated_job.status}\")\n",
    "\n",
    "        # Wait for completion (polling)\n",
    "        # Uncomment to wait for job completion:\n",
    "        # final_job = client.jobs.wait_for_completion(job.id, timeout=300)\n",
    "        # if final_job.status == \"completed\":\n",
    "        #     # Results are embedded in the job resource\n",
    "        #     print(f\"✓ Job completed with results\")\n",
    "\n",
    "    except NotImplementedError:\n",
    "        print(\"✗ Job submission not yet implemented (skeleton only)\")\n",
    "    except Exception as e:\n",
    "        print(f\"✗ Failed to submit job: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 4: Async Client Usage\n",
    "\n",
    "Using the asynchronous client for better performance with I/O-bound operations.\n",
    "\n",
    "**Note:** Same method names as sync - just await them!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def async_example():\n",
    "    \"\"\"Demonstrate async client usage.\"\"\"\n",
    "    async with AsyncEvalHubClient() as client:\n",
    "        try:\n",
    "            # Async health check - same method name!\n",
    "            health = await client.health()\n",
    "            print(f\"✓ Async health check: {health}\")\n",
    "\n",
    "            # Async provider list - same method name!\n",
    "            providers = await client.providers.list()\n",
    "            print(f\"✓ Found {len(providers)} providers (async)\")\n",
    "\n",
    "            # Async job submission - same method name!\n",
    "            # Using a vLLM endpoint deployed on OpenShift\n",
    "            request = EvaluationRequest(\n",
    "                benchmark_id=\"mmlu\",\n",
    "                model=ModelConfig(\n",
    "                    url=\"http://vllm-service.my-namespace.svc.cluster.local:8000/v1\",\n",
    "                    name=\"meta-llama/Llama-2-7b-chat-hf\",\n",
    "                ),\n",
    "            )\n",
    "            # Uncomment to submit job:\n",
    "            # job = await client.jobs.submit(request)\n",
    "            # print(f\"✓ Async job submitted: {job.id}\")\n",
    "            \n",
    "            # You can also wait for completion asynchronously\n",
    "            # final_job = await client.jobs.wait_for_completion(job.id, timeout=300)\n",
    "            # if final_job.status == \"completed\":\n",
    "            #     # Results are embedded in the job resource\n",
    "            #     print(f\"✓ Job completed with results\")\n",
    "\n",
    "        except NotImplementedError:\n",
    "            print(\"✗ Some async operations not yet implemented (skeleton only)\")\n",
    "        except Exception as e:\n",
    "            print(f\"✗ Async operation failed: {e}\")\n",
    "\n",
    "# Run the async example\n",
    "try:\n",
    "    await async_example()\n",
    "except Exception as e:\n",
    "    print(f\"✗ Failed to run async example: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 5: Client Class Comparison\n",
    "\n",
    "### Sync vs Async - Same method names!\n",
    "\n",
    "**Synchronous (SyncEvalHubClient):**\n",
    "```python\n",
    "with SyncEvalHubClient() as client:\n",
    "    providers = client.list()           # No await needed\n",
    "    job = client.submit(request)        # No await needed\n",
    "    status = client.get_job(job_id)     # No await needed\n",
    "```\n",
    "\n",
    "**Asynchronous (AsyncEvalHubClient):**\n",
    "```python\n",
    "async with AsyncEvalHubClient() as client:\n",
    "    providers = await client.list()     # Await needed\n",
    "    job = await client.submit(request)  # Await needed\n",
    "    status = await client.get_job(job_id)  # Await needed\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Features\n",
    "\n",
    "### List Collections\n",
    "\n",
    "Collections are pre-defined sets of benchmarks for specific use cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with SyncEvalHubClient() as client:\n",
    "    try:\n",
    "        collections = client.collections.list()\n",
    "        print(f\"Available collections: {len(collections)}\")\n",
    "        for collection in collections:\n",
    "            print(f\"  - {collection.name}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to list collections: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter Benchmarks by Provider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with SyncEvalHubClient() as client:\n",
    "    try:\n",
    "        # Get benchmarks from a specific provider\n",
    "        benchmarks = client.benchmarks.list(\n",
    "            provider_id=\"lm_evaluation_harness\",\n",
    "            limit=5\n",
    "        )\n",
    "        print(f\"Found {len(benchmarks)} benchmarks\")\n",
    "        for benchmark in benchmarks:\n",
    "            print(f\"  - {benchmark.benchmark_id}: {benchmark.name}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to filter benchmarks: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
