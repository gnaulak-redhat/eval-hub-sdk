provider_id: lm_evaluation_harness
provider_name: LM Evaluation Harness
description: Comprehensive evaluation framework for language models with 167 benchmarks
provider_type: builtin
base_url: null
runtime:
  k8s:
    image: quay.io/eval-hub/lm_evaluation_harness:latest
    entrypoint:
      - python
      - /opt/app-root/src/main.py
    cpu_request: 100m
    memory_request: 128Mi
    cpu_limit: 500m
    memory_limit: 1Gi
    env:
      - name: VAR_NAME
        value: VALUE
  local:
    # reserved for local runtime

benchmarks:
  - benchmark_id: arc_easy
    name: ARC Easy
    description: ARC Easy evaluation benchmark - AI2 Reasoning Challenge (Easy)
    category: reasoning
    metrics:
      - accuracy
      - acc_norm
    num_few_shot: 0
    dataset_size: 2376
    tags:
      - reasoning
      - science
      - lm_eval
  - benchmark_id: AraDiCE_boolq_lev
    name: Aradice Boolq Lev
    description: Aradice Boolq Lev evaluation benchmark
    category: general
    metrics:
      - accuracy
    num_few_shot: 0
    dataset_size: 3270
    tags:
      - general
      - lm_eval
  - benchmark_id: blimp
    name: Blimp
    description: Blimp evaluation benchmark
    category: general
    metrics:
      - accuracy
    num_few_shot: 0
    dataset_size: 1000
    tags:
      - general
      - lm_eval
# simplify testing by only testing few benchmarks...
