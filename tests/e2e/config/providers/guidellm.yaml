id: guidellm
name: GuideLLM
description: Performance benchmarking framework for LLM inference servers
type: builtin
runtime:
  k8s:
    image: quay.io/evalhub/community-guidellm:latest
    entrypoint:
      - python
      - main.py
    cpu_request: 100m
    memory_request: 128Mi
    cpu_limit: 1000m
    memory_limit: 2Gi
  local:
    # reserved for local runtime

benchmarks:
  # Execution profiles - comprehensive performance testing
  - id: sweep
    name: Sweep Profile
    description: Automatically finds optimal request rate by sweeping from low to high concurrency
    category: performance
    metrics:
      - requests_per_second
      - prompt_tokens_per_second
      - output_tokens_per_second
      - mean_ttft_ms
      - mean_itl_ms
    tags:
      - performance
      - throughput
      - latency
      - guidellm
      - auto

  - id: throughput
    name: Throughput Profile
    description: Measures maximum throughput by gradually increasing request rate until saturation
    category: performance
    metrics:
      - requests_per_second
      - prompt_tokens_per_second
      - output_tokens_per_second
      - mean_ttft_ms
      - mean_itl_ms
    tags:
      - performance
      - throughput
      - guidellm
      - saturation

  - id: concurrent
    name: Concurrent Profile
    description: Tests performance with fixed number of concurrent requests
    category: performance
    metrics:
      - requests_per_second
      - prompt_tokens_per_second
      - output_tokens_per_second
      - mean_ttft_ms
      - mean_itl_ms
    tags:
      - performance
      - concurrency
      - guidellm
# truncated for E2E testing purposes.
